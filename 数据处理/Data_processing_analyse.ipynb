{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a demo for data integration、processing and analysis'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "##                       _oo0oo_\n",
    "##                      o8888888o\n",
    "##                      88\" . \"88\n",
    "##                      (| -_- |)\n",
    "##                      0\\  =  /0\n",
    "##                    ___/`---'\\___\n",
    "##                  .' \\\\|     |// '.\n",
    "##                 / \\\\|||  :  |||// \\\n",
    "##                / _||||| -:- |||||- \\\n",
    "##               |   | \\\\\\  -  /// |   |\n",
    "##               | \\_|  ''\\---/''  |_/ |\n",
    "##               \\  .-\\__  '-'  ___/-. /\n",
    "##             ___'. .'  /--.--\\  `. .'___\n",
    "##          .\"\" '<  `.___\\_<|>_/___.' >' \"\".\n",
    "##         | | :  `- \\`.;`\\ _ /`;.`/ - ` : | |\n",
    "##         \\  \\ `_.   \\_ __\\ /__ _/   .-` /  /\n",
    "##     =====`-.____`.___ \\_____/___.-`___.-'=====\n",
    "##                       `=---='\n",
    "##     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "##              佛祖保佑 永无BUG 永不修改 \n",
    "##         本项目已经过开光处理，绝无可能再出现bug! \n",
    "# -*- coding: utf-8 -*-\n",
    "# C:\\Users\\Administrator\\OneDrive python\n",
    "\"this is a demo for data integration、processing and analysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  导入模块\n",
    "import os\n",
    "import re\n",
    "import pymysql\n",
    "import datetime\n",
    "import time\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import calendar\n",
    "import threading\n",
    "import matplotlib\n",
    "from random import randint\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.fftpack import fft\n",
    "from scipy import interpolate\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  全局变量\n",
    "host = \"localhost\"\n",
    "user = \"root\"\n",
    "password = \"941130zmy\"\n",
    "\n",
    "FREQ_REFERENCE = {\"fx\":500000,\"dyb\":20000,\"jsd\":10000,\"dqw\":500000,\"sd\":1000000,\\\n",
    "                 \"fs\":500000,\"wy\":1000000}    # 频率对照表\n",
    "package_number = 13                        # 包初始标识号\n",
    "#file_path_name = \"C:/Users/Administrator/Desktop/SMJ_DYB-03-01_010000_013402.DY.7z\"    # 传入的压缩包文件的绝对路径\n",
    "# 用于桥面名匹配文件名中桥梁代号的字典\n",
    "dictionary01 = {\"白露大桥\":\"BL\",\"文惠桥.新桥\":\"WHN\",\"文惠桥.老桥\":\"WHO\",\"壶东大桥\":\"DH\",\"三门江大桥\":\"SMJ\",\"鹧鸪江大桥\":\"ZGJ\"}\n",
    "dictionary02 = {\"bl\":\"01\",\"whn\":\"02\",\"who\":\"03\",\"hd\":\"04\",\"smj\":\"05\",\"zgj\":\"06\"}\n",
    "dictionary03 = {\"BL\":\"白露大桥\",\"WHN\":\"文惠桥.新桥\",\"WHO\":\"文惠桥.老桥\",\"DH\":\"壶东大桥\",\"SMJ\":\"三门江大桥\",\"SGJ\":\"鹧鸪江大桥\"}\n",
    "dictionary04 = {\"dyb\":1000,\"dqw\":999,\"jsd\":998}\n",
    "# 用于将数字1,2...转为01,02...的list\n",
    "number = [1,2,3,4,5,6,7,8,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  节点类\n",
    "class BridgeNode_Info(object):\n",
    "    def __init__(self,bus_nodeconfig):\n",
    "        try:\n",
    "            self.__NodeID = bus_nodeconfig[0]\n",
    "            self.__BridgeNoID = bus_nodeconfig[1]\n",
    "            self.__CheckTypeID = bus_nodeconfig[2]\n",
    "            self.__CheckItemsID = bus_nodeconfig[3]\n",
    "            self.__SensorTypeID = bus_nodeconfig[4]\n",
    "            self.__PartsTypeID = bus_nodeconfig[5]\n",
    "            self.__NodeCodeNo = bus_nodeconfig[6]\n",
    "            self.__MonitorNo = bus_nodeconfig[7]\n",
    "            self.__SectionNo = bus_nodeconfig[8]\n",
    "            self.__SensorNum = bus_nodeconfig[9]\n",
    "            self.__SensorNo = bus_nodeconfig[10]\n",
    "            self.__ThresholdUpper = bus_nodeconfig[11]\n",
    "            self.__ThresholdLower = bus_nodeconfig[12]\n",
    "            self.__AccuracyUpper = bus_nodeconfig[13]\n",
    "            self.__AccuracyLower = bus_nodeconfig[14]\n",
    "            self.__Frequency = bus_nodeconfig[15]\n",
    "            self.__FilePath = bus_nodeconfig[16]\n",
    "            self.__DataFile = bus_nodeconfig[17]\n",
    "            self.__DataStructure = bus_nodeconfig[18]\n",
    "            self.__DataUnit = bus_nodeconfig[19]\n",
    "            self.__PartsUpper = bus_nodeconfig[20]\n",
    "            self.__PartsLower = bus_nodeconfig[21]\n",
    "            self.__Explains = bus_nodeconfig[22]\n",
    "            self.__DefaultField1 = bus_nodeconfig[23]\n",
    "            self.__DefaultField2 = bus_nodeconfig[24]\n",
    "            self.__DefaultField3 = bus_nodeconfig[25]\n",
    "            self.__RecName = bus_nodeconfig[26]\n",
    "            self.__RecTime = bus_nodeconfig[27]\n",
    "            self.__RecStatus = bus_nodeconfig[28]\n",
    "            self.__Remark = bus_nodeconfig[29]\n",
    "        except:\n",
    "            print(\"节点配置初始化失败！\")\n",
    "            return \"wrong\"\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    def get_NodeId(self):\n",
    "        return self.__NodeID\n",
    "    def get_BridgeNoID(self):\n",
    "        return self.__BridgeNoID\n",
    "    def get_CheckTypeID(self):\n",
    "        return self.__CheckTypeID\n",
    "    def get_CheckItemsID(self):\n",
    "        return self.__CheckItemsID\n",
    "    def get_SensorTypeID(self):\n",
    "        return self.__SensorTypeID\n",
    "    def get_PartsTypeID(self):\n",
    "        return self.PartsTypeID\n",
    "    def get_NodeCodeNo(self):\n",
    "        return self.__NodeCodeNo\n",
    "    def get_MonitorNo(self):\n",
    "        return self.__MonitorNo\n",
    "    def get_SectionNo(self):\n",
    "        return self.__SectionNo\n",
    "    def get_SensorNum(self):\n",
    "        return self.__SensorNum\n",
    "    def get_SensorNo(self):\n",
    "        return self.__SensorNo\n",
    "    def get_ThresholdUpper(self):\n",
    "        return self.__ThresholdUpper\n",
    "    def get_ThresholdLower(self):\n",
    "        return self.__ThresholdLower\n",
    "    def get_AccuracyUpper(self):\n",
    "        return self.__AccuracyUpper\n",
    "    def get_AccuracyLower(self):\n",
    "        return self.__AccuracyLower\n",
    "    def get_Frequency(self):\n",
    "        return self.__Frequency\n",
    "    def get_FilePath(self):\n",
    "        return self.__FilePath\n",
    "    def get_DataFile(self):\n",
    "        return self.__DataFile\n",
    "    def get_DataStructure(self):\n",
    "        return self.__DataStructure\n",
    "    def get_DataUnit(self):\n",
    "        return self.__DataUnit\n",
    "    def get_PartsUpper(self):\n",
    "        return self.__PartsUpper\n",
    "    def get_PartsLower(self):\n",
    "        return self.__PartsLower\n",
    "    def get_Explains(self):\n",
    "        return self.__Explains\n",
    "    def get_RecName(self):\n",
    "        return self.__RecName\n",
    "    def get_RecTime(self):\n",
    "        return self.__RecTime\n",
    "    def get_RecStatus(self):\n",
    "        return self.__RecStatus\n",
    "    def get_Remark(self):\n",
    "        return self.__Remark\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  打开数据库\n",
    "##  输入：mysql基础信息，其中host、user、psd必填，库名、端口、编码可以使用默认值\n",
    "##  输出：mysql操作对象conn\n",
    "def connect_mysql(my_host,my_user,my_password,my_database = \"bridge\",my_port = 3306,my_charset = \"utf8\"):\n",
    "    try:\n",
    "        conn =  pymysql.connect(\n",
    "            host = my_host,\n",
    "            user = my_user,\n",
    "            passwd = my_password,\n",
    "            db = my_database,\n",
    "            port = my_port,\n",
    "            charset= my_charset\n",
    "            )\n",
    "        return conn\n",
    "    except:\n",
    "        print(\"数据库连接失败！请核对信息重新连接！\")\n",
    "        return \"wrong\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  关闭数据库  \n",
    "##  输入：mysql操作对象 conn\n",
    "def close_mysql(conn):\n",
    "    try:\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "    except:\n",
    "        print(\"关闭数据库操作对象失败！\")\n",
    "        return \"wrong\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  判断数据表是否存在\n",
    "##  输入：数据库对象conn，表名table_name\n",
    "##  输出：存在返回True，否则返回False\n",
    "def table_exist(conn,table_name):\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        sql = \"show tables;\"\n",
    "        cur.execute(sql)\n",
    "        # 将命令运行获取的库中所有表名存在table_list中\n",
    "        tables = [cur.fetchall()]\n",
    "        table_list = re.findall('(\\'.*?\\')',str(tables))\n",
    "        table_list = [re.sub(\"'\",'',each) for each in table_list]    # 将表名的点引号删掉\n",
    "        cur.close()    # 关闭命令接口\n",
    "        conn.commit()    # 提交\n",
    "    except:\n",
    "        print(\"查找数据表出错！\")\n",
    "        return \"wrong\"\n",
    "    else:\n",
    "        if table_name in table_list:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  创建丢包记录表\n",
    "##  输入：数据库操作对象conn，欲建表名table_name\n",
    "def create_lost_package_table(conn,table_name):\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        create_sql = \"CREATE TABLE `\" + table_name + \"` (ID INT unsigned not null AUTO_INCREMENT COMMENT '记序', \\\n",
    "                    BridgeName_NodeID VARCHAR(100) not null COMMENT '唯一标识', Last_lost_time DATETIME not null COMMENT '上次丢失时间'\\\n",
    "                    ,Lost_num INT unsigned not null COMMENT '连续丢包数',Alarm_flag INT unsigned not null COMMENT '报警标志',primary key(ID))ENGINE = InnoDB DEFAULT CHARSET = UTF8MB4;\"\n",
    "        cur.execute(create_sql)\n",
    "        cur.close()\n",
    "        conn.commit()\n",
    "    except:\n",
    "        print(\"创建丢包记录表错误！\")\n",
    "        return \"wrong\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  从丢包记录表查找记录\n",
    "##  输入：数据库操作对象 conn，桥梁+节点名 lost_info，表名 table_name=lost_package_table\n",
    "##  输出：查询结果\n",
    "def inquire_lost_info(conn,lost_info,table_name = \"lost_package_table\"):\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        inquire_sql = \"SELECT * FROM \" + table_name + \" WHERE BridgeName_NodeID = \\'\" + lost_info + \"\\'\" \n",
    "        cur.execute(inquire_sql)\n",
    "        cur.close()\n",
    "        conn.commit()\n",
    "        return cur.fetchall()\n",
    "    except:\n",
    "        print(\"查找丢包记录失败！\")\n",
    "        return \"wrong\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  向丢包记录表插入记录\n",
    "##  输入：数据库操作对象 conn，桥梁+节点名 lost_info，丢包数 lost_num，标志 lost_flag，表名 table_name\n",
    "def insert_into_lost_table(conn,lost_info,lost_num,lost_flag,table_name = \"lost_package_table\"):\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        insert_sql = \"INSERT INTO \" + table_name + \" (BridgeName_NodeID,Last_lost_time,Lost_num,Alarm_flag) VALUES(\\\"%s\\\",\\\"%s\\\",\\\"%s\\\",\\\"%s\\\")\"\\\n",
    "                    %(lost_info,datetime.datetime.now(),lost_num,lost_flag)\n",
    "        cur.execute(insert_sql)\n",
    "        cur.close()\n",
    "        conn.commit()\n",
    "    except:\n",
    "        print(\"插入丢包记录失败！\")\n",
    "        return \"wrong\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  修改丢包记录表\n",
    "##  输入：数据库操作对象conn，桥梁+节点名 lost_info，丢包数 lost_num，标志 lost_flag，表名 table_name\n",
    "def update_lost_table(conn,lost_info,lost_num,lost_flag,table_name = \"lost_package_table\"):\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        update_sql = \"UPDATE \" + table_name + \" SET Last_lost_time = \\'\" + datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \\\n",
    "                    \"\\',Lost_num = \" + str(lost_num) + \",Alarm_flag = \" \\\n",
    "                + str(lost_flag) + \" WHERE BridgeName_NodeID = \\'\" + lost_info + \"\\'\"\n",
    "        cur.execute(update_sql)\n",
    "        cur.close()\n",
    "        conn.commit()\n",
    "    except:\n",
    "        print(\"修改丢包记录失败！\")\n",
    "        return \"wrong\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  创建原始数据数据表\n",
    "##  输入：数据库操作对象conn，欲建表名table_name\n",
    "def create_data_table(conn,table_name):\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        '''\n",
    "        # 创建带有桥梁节点信息的数据表\n",
    "        create_sql = \"CREATE TABLE \" + table_name + \" (ID INT unsigned not null AUTO_INCREMENT, \\\n",
    "                    Bridge_Name VARCHAR(20) not null, Node_Name VARCHAR(20) not null, \\\n",
    "                    Date_Time DATETIME not null, Detect_Value FLOAT(10,4), \\\n",
    "                    primary key(ID))ENGINE = InnoDB CHARSET = UTF8MB4;\" \n",
    "        '''\n",
    "        # 创建包含ID、Date_Time、Detect_Value、Package_Number的数据表\n",
    "        create_sql = \"CREATE TABLE `\" + table_name + \"` (ID INT unsigned not null AUTO_INCREMENT, \\\n",
    "                    Date_Time DATETIME not null, Detect_Value FLOAT(10,4),\\\n",
    "                    Package_Number INT unsigned not null,primary key(ID))ENGINE = InnoDB DEFAULT CHARSET = UTF8MB4;\"\n",
    "        cur.execute(create_sql)\n",
    "        cur.close()\n",
    "        conn.commit()\n",
    "    except:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  向数据表中插入数据\n",
    "##  输入：数据库操作对象conn，欲插入的表名table_name，插入的数据data，数据起始时间begin_time，该类型传感器采样频率freq，包号package_number\n",
    "def insert_into_original_table(conn,table_name,data,begin_time,freq,package_number):\n",
    "    cur = conn.cursor()\n",
    "    package_number = str(package_number)\n",
    "    record_time = begin_time\n",
    "    timedelta = datetime.timedelta(microseconds = freq)\n",
    "    insert_sql = \"INSERT INTO \" + table_name + \" (Date_Time,Detect_Value,Package_Number) VALUES \"\n",
    "    for d in data[0:-1]:\n",
    "        insert_data = \"(\\\"\" + str(record_time) + \"\\\",\" + str(d) + \",\" + package_number +\"),\"\n",
    "        insert_sql += insert_data\n",
    "        record_time = record_time + timedelta\n",
    "    insert_sql += \"(\\\"\" + str(record_time) + \"\\\",\" + str(data[len(data)-1]) + \",\" + package_number + \");\"\n",
    "    n = cur.execute(insert_sql)\n",
    "    cur.close()\n",
    "    conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  创建预处理后数据表\n",
    "##  输入：数据库操作对象conn，欲建表名table_name\n",
    "def create_alterdata_table(conn,table_name):\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        '''\n",
    "        create_sql = \"CREATE TABLE \" + table_name + \" (ID INT unsigned not null AUTO_INCREMENT COMMENT '数据ID', \\\n",
    "                    Bridge_Name VARCHAR(20) not null COMMENT '桥梁名称',Bridge_ID INT(10) not null COMMENT '桥梁编号',\\\n",
    "                    Node_Name VARCHAR(20) not null COMMENT '节点名称', Date_Time DATETIME not null COMMENT '检测时间',\\\n",
    "                    Detect_Value FLOAT(10,4) COMMENT '检测值',primary key(ID))ENGINE = InnoDB CHARSET = UTF8MB4;\" \n",
    "        '''\n",
    "        create_sql = \"CREATE TABLE `\" + table_name + \"` (ID INT unsigned not null AUTO_INCREMENT COMMENT '数据ID 自增长主键', \\\n",
    "                    Date_Time DATETIME(3) not null COMMENT '检测时间',Treat_Value FLOAT(10,4) COMMENT '预处理后的值',\\\n",
    "                    Package_Number INT unsigned not null COMMENT '所属包编号',primary key(ID))ENGINE = InnoDB DEFAULT CHARSET = UTF8MB4;\"\n",
    "        cur.execute(create_sql)\n",
    "        cur.close()\n",
    "        conn.commit()\n",
    "    except:\n",
    "        print(\"创建预处理后数据表失败！\")\n",
    "        return \"wrong\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  向预处理后数据表中插入数据\n",
    "##  输入：数据库操作对象conn，欲插入的表名table_name，插入的数据data，数据起始时间begin_time，该类型传感器采样频率freq，包号package_number\n",
    "##        data: pandas.Series类型，只存储数据值\n",
    "def insert_into_alter_table(conn,table_name,data,begin_time,freq,package_number):\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        package_number = str(package_number)\n",
    "        record_time = begin_time\n",
    "        # 将采样频率转换为数据间隔时间\n",
    "        timedelta = datetime.timedelta(microseconds = freq)\n",
    "        # 一次insert指令插入多个记录\n",
    "        insert_sql = \"INSERT INTO `\" + table_name + \"` (Date_Time,Treat_Value,Package_Number) VALUES \"\n",
    "        for d in data[0:-1]:\n",
    "            insert_data = \"(\\\"\" + str(record_time) + \"\\\",\" + str(d) + \",\" + package_number +\"),\"\n",
    "            insert_sql += insert_data\n",
    "            record_time = record_time + timedelta\n",
    "        insert_sql += \"(\\\"\" + str(record_time) + \"\\\",\" + str(data[len(data)-1]) + \",\" + package_number + \");\"\n",
    "        num = cur.execute(insert_sql)    # 插入的记录数 如果和len（data）一致则无问题\n",
    "        cur.close()\n",
    "        # 提交处理 不然不会实时与数据库同步\n",
    "        conn.commit()\n",
    "    except:\n",
    "        print(\"插入预处理数据失败！\")\n",
    "        return \"wrong\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  创建质量分析报告表 一个节点对应一张表\n",
    "##  输入：数据库操作对象conn，欲建表名table_name\n",
    "def create_report_table(conn,table_name):\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        create_sql = \"CREATE TABLE `\" + table_name + \"`(ID INT(10) unsigned not null AUTO_INCREMENT comment '序号',Package_ID INT unsigned not null comment '包编号--主键',Bridge_Name VARCHAR(20) not null comment '桥梁名称',\\\n",
    "                    Bridge_ID INT(10) not null comment '桥梁编号',Node_Name VARCHAR(20) not null comment '节点名称',Report_Date DATETIME not null comment '报告查询时间',\\\n",
    "                    count_sum INT unsigned not null comment '数据总量',count_not_null INT unsigned not null comment '非空数据量',\\\n",
    "                    mean_quality FLOAT(10,4) not null comment '数据均值',std_quality FLOAT(10,4) not null comment '数据标准差',\\\n",
    "                    max_quality FLOAT(10,4) not null comment '数据最大值',min_quality FLOAT(10,4) not null comment '数据最小值',\\\n",
    "                    at_0_25_percent FLOAT not null comment '分布在前25%的数据量',at_25_50_percent FLOAT not null comment '分布在25~50%数据量',\\\n",
    "                    at_50_75_percent FLOAT not null comment '分布在50~75%数据量',at_75_100_percent FLOAT not null comment '分布在后25%数据量',positive_num INT unsigned not null comment '正值数量',\\\n",
    "                    negative_num INT unsigned not null comment '负值数量',positive_percent FLOAT not null comment '正值占比',\\\n",
    "                    negative_percent FLOAT not null comment '负值占比',abnormal_num INT unsigned not null comment '异常值数量',\\\n",
    "                    abnormal_percent FLOAT not null comment '异常值占比',null_percent FLOAT not null comment '空值占比',\\\n",
    "                    jicha FLOAT(10,4) not null comment '极差',zhongshu FLOAT(10,4) not null comment '众数',\\\n",
    "                    skewness FLOAT(10,4) not null comment '偏度',overall_quality FLOAT(10,4) not null comment '该包数据总体质量',abnormal_judge INT DEFAULT 1 comment '包异常情况判断',primary key(ID))\\\n",
    "                    ENGINE = InnoDB DEFAULT CHARSET = UTF8MB4;\"\n",
    "        cur.execute(create_sql)\n",
    "        cur.close()\n",
    "        conn.commit()\n",
    "    except:\n",
    "        print(\"创建质量分析报告失败！\")\n",
    "        return \"wrong\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  向报告表插入包报告\n",
    "##  输入：数据库操作对象conn，桥名bridge_name，节点名node_name，日期date，\n",
    "##        统计分析结果sta_analyse（Series），值分析结果value_analyse（list），包号package_number，缺省的报告表名table_name\n",
    "def insert_into_reporttable(conn,bridge_name,node_name,date,sta_analyse,value_analyse,package_number,table_name):\n",
    "    cur = conn.cursor()\n",
    "    insert_sql = \"INSERT INTO `\" + table_name + \"` (Package_ID,Bridge_Name,Bridge_ID,Node_Name,Report_Date,count_sum,count_not_null,\\\n",
    "            mean_quality,std_quality,max_quality,min_quality,at_0_25_percent,at_25_50_percent,at_50_75_percent,at_75_100_percent,positive_num,negative_num,\\\n",
    "            positive_percent,negative_percent,abnormal_num,abnormal_percent,null_percent,jicha,zhongshu,skewness,overall_quality)\\\n",
    "            VALUES (\\'%s\\',\\'%s\\',\\'%s\\',\\'%s\\',\\'%s\\',\\'%s\\',\\\n",
    "            \\'%s\\',\\'%s\\',\\'%s\\',\\'%s\\',\\'%s\\',\\'%s\\',\\'%s\\',\\'%s\\',\\'%s\\',\\'%s\\',\\'%s\\',\\\n",
    "            \\'%s\\',\\'%s\\',\\'%s\\',\\'%s\\',\\'%s\\',\\'%s\\',\\'%s\\',\\'%s\\',\\'%s\\');\"%(package_number,\\\n",
    "            bridge_name,dictionary02.get(bridge_name),node_name,date,value_analyse[-1],value_analyse[0],sta_analyse[1],\\\n",
    "            sta_analyse[2],sta_analyse[7],sta_analyse[3],value_analyse[11],value_analyse[12],\\\n",
    "            value_analyse[13],value_analyse[14],value_analyse[1],value_analyse[2],value_analyse[3],\\\n",
    "            value_analyse[4],value_analyse[5],value_analyse[6],value_analyse[-2]/value_analyse[-1],value_analyse[7],\\\n",
    "            value_analyse[8],value_analyse[9],value_analyse[10])\n",
    "    n = cur.execute(insert_sql)    # 一次插入一条记录\n",
    "    cur.close()\n",
    "    conn.commit()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  创建分布直方图绘图数据表\n",
    "##  输入：数据库操作对象conn，欲建表名table_name\n",
    "def create_dist_hist_table(conn,table_name = \"dist_hist_report\"):\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        create_sql = \"CREATE TABLE \" + table_name + \"(ID INT unsigned not null AUTO_INCREMENT comment '记录自增主键',Bridge_Name VARCHAR(20) not null comment '桥梁名称',\\\n",
    "                    Bridge_ID INT(10) not null COMMENT '桥梁编号',Node_Name VARCHAR(20) not null comment '节点名称',date_inquery VARCHAR(50) not null comment '该条记录数据对应的日期',\\\n",
    "                    hist_x VARCHAR(200) not null comment '横坐标的列表',hist_y VARCHAR(200) not null comment '纵坐标的列表',PRIMARY KEY(ID))\\\n",
    "                    ENGINE = InnoDB DEFAULT CHARSET = UTF8MB4;\"\n",
    "        cur.execute(create_sql)\n",
    "        cur.close()\n",
    "        conn.commit()\n",
    "    except:\n",
    "        print(\"创建数据分布直方图绘图数据表失败！\")\n",
    "        return \"wrong\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  分布直方图绘图数据表中插入数据\n",
    "##  输入：数据库操作对象conn，桥名bridge_name，节点名node_name，日期date，\n",
    "##         直方图横坐标列表hist_x，直方图纵坐标列表hist_y，表名table_name\n",
    "def insert_into_dist_hist(conn,bridge_name,node_name,date,hist_x,hist_y,table_name = \"dist_hist_report\"):\n",
    "    try:\n",
    "        tmp_x = \"\"    # 将list转换为str\n",
    "        tmp_y = \"\"\n",
    "        for x in hist_x:\n",
    "            if tmp_x == \"\":\n",
    "                tmp_x = str(x)\n",
    "            else:\n",
    "                tmp_x += \",\" + str(x)\n",
    "        for y in hist_y:\n",
    "            if tmp_y == \"\":\n",
    "                tmp_y = str(y)\n",
    "            else:\n",
    "                tmp_y += \",\" + str(y)\n",
    "        cur = conn.cursor()\n",
    "        insert_sql = \"INSERT INTO \" + table_name + \" (Bridge_Name,Bridge_ID,Node_Name,date_inquery,hist_x,hist_y)VALUES(\\'%s\\',\\'%s\\',\\'%s\\',\\'%s\\',\\'%s\\',\\'%s\\');\"\\\n",
    "                %(bridge_name,dictionary02.get(bridge_name),node_name,date,tmp_x,tmp_y)\n",
    "        cur.execute(insert_sql)\n",
    "        cur.close()\n",
    "        conn.commit()\n",
    "    except:\n",
    "        print(\"插入数据分布直方图绘图数据失败！\")\n",
    "        return \"wrong\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  创建统计直方图绘图数据表\n",
    "##  输入：数据库操作对象conn，欲建表名table_name\n",
    "def create_stat_hist_table(conn,table_name = \"stat_hist_report\"):\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        create_sql = \"CREATE TABLE \" + table_name + \"(ID INT unsigned not null AUTO_INCREMENT comment '记录自增主键',Bridge_Name VARCHAR(20) not null comment '桥梁名称',Bridge_ID INT(10) not null comment '桥梁编号',\\\n",
    "                    Node_Name VARCHAR(20) not null comment '节点名称',date_inquery VARCHAR(50) not null comment '该条记录对应的数据日期',\\\n",
    "                    mean_value FLOAT(10,4) not null comment '均值',std_value FLOAT(10,4) not null comment '标准差',jicha FLOAT(10,4) not null comment '极差',skew FLOAT(10,4)not null comment '偏度'\\\n",
    "                    ,PRIMARY KEY(ID))ENGINE = InnoDB DEFAULT CHARSET = UTF8MB4;\"\n",
    "        cur.execute(create_sql)\n",
    "        cur.close()\n",
    "        conn.commit()\n",
    "    except:\n",
    "        print(\"创建统计直方图绘图数据表失败！\")\n",
    "        return \"wrong\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  统计直方图绘图数据表中插入数据\n",
    "##  输入：数据库操作对象conn，桥名bridge_name，节点名node_name，日期date，数据df，统计结果data_info，表名table_name\n",
    "def insert_into_stat_hist(conn,bridge_name,node_name,date,df,data_info,table_name = \"stat_hist_report\"):\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        insert_sql = \"INSERT INTO \" + table_name + \" (Bridge_Name,Bridge_ID,Node_Name,date_inquery,mean_value,std_value,jicha,skew)\\\n",
    "                VALUES(\\'%s\\',\\'%s\\',\\'%s\\',\\'%s\\',\\'%s\\',\\'%s\\',\\'%s\\',\\'%s\\');\"%(bridge_name,dictionary02.get(bridge_name),node_name,date,data_info[1],data_info[2],data_info[7]-data_info[3],df[\"value\"].skew())\n",
    "        cur.execute(insert_sql)\n",
    "        cur.close()\n",
    "        conn.commit()\n",
    "    except:\n",
    "        print(\"插入统计直方图绘图数据失败！\")\n",
    "        return \"wrong\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  创建分布饼图绘图数据表\n",
    "##  输入：数据库操作对象conn，表名table_name\n",
    "def create_dist_pie_table(conn,table_name = \"dist_pie_report\"):\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        create_sql = \"create table \" + table_name + \"(ID INT(100) not null AUTO_INCREMENT comment '记录自增主键',bridge_name varchar(50) not null comment '桥梁名称',bridge_ID INT(10) not null comment '桥梁编号',\\\n",
    "                node_name varchar(50) not null comment '节点名称',date_inquery varchar(50) not null comment '数据对应日期',a_perc float(10,4) not null comment 'a部分占比',b_perc float(10,4) not null comment 'b部分占比',\\\n",
    "                c_perc float(10,4) not null comment 'c部分占比',d_perc float(10,4) not null comment 'd部分占比',a_label varchar(50) not null comment 'a部分范围',b_label varchar(50) not null comment 'b部分范围',\\\n",
    "                c_label varchar(50) not null comment 'c部分范围',d_label varchar(50) not null comment 'd部分范围',primary key(id))ENGINE = InnoDB DEFAULT CHARSET=UTF8MB4;\"\n",
    "        cur.execute(create_sql)\n",
    "        cur.close()\n",
    "        conn.commit()\n",
    "    except:\n",
    "        print(\"创建分布饼图绘图数据表失败！\")\n",
    "        return \"wrong\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  分布饼图数据表中插入数据\n",
    "##  输入：数据库操作对象conn，表名bridge_name,节点名node_name,查询日期date,饼图数据perc,对应标签labels,表名table_name\n",
    "def insert_into_dist_pie(conn,bridge_name,node_name,date,perc,labels,table_name = \"dist_pie_report\"):\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        insert_sql = \"insert into \" + table_name + \"(bridge_name,bridge_id,node_name,date_inquery,a_perc,b_perc,c_perc,d_perc,a_label,b_label,\\\n",
    "                c_label,d_label)VALUES(\\'%s\\',\\'%s\\',\\'%s\\',\\'%s\\',\\'%s\\',\\'%s\\',\\'%s\\',\\'%s\\',\\'%s\\',\\'%s\\',\\'%s\\',\\'%s\\')\"\\\n",
    "                %(bridge_name,dictionary02.get(bridge_name),node_name,date,perc[0],perc[1],perc[2],perc[3],labels[0],labels[1],labels[2],labels[3])\n",
    "        cur.execute(insert_sql)\n",
    "        cur.close()\n",
    "        conn.commit()\n",
    "    except:\n",
    "        print(\"插入分布饼图绘图数据失败！\")\n",
    "        return \"wrong\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  创建质量分析饼图绘图数据表\n",
    "##  输入：数据库操作对象conn，表名table_name\n",
    "def create_qual_pie_table(conn,table_name = \"qual_pie_report\"):\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        create_sql = \"create table \" + table_name + \"(ID INT(100) not null AUTO_INCREMENT comment '记录自增主键',bridge_name varchar(50) not null comment '桥梁名称',\\\n",
    "                bridge_id int(10) not null comment '桥梁编号',node_name varchar(50) not null comment '节点名称',date_inquery varchar(50) not null comment '数据对应日期',\\\n",
    "                data_num INT(200) not null comment '数据总量',execent_num INT(100) comment '优秀品质数据量',good_num INT(100) comment '良好品质数据量',\\\n",
    "                soso_num INT(100) comment '一般品质数据量',bad_num INT(100) comment '差品质数据量',worse_num INT(100) comment '糟糕品质数据量',primary key(id))ENGINE = InnoDB DEFAULT CHARSET=UTF8MB4;\"\n",
    "        cur.execute(create_sql)\n",
    "        cur.close()\n",
    "        conn.commit()\n",
    "    except:\n",
    "        print(\"创建质量分析饼图绘图数据表失败！\")\n",
    "        return \"wrong\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  质量饼图数据表中插入数据\n",
    "##  输入：数据库操作对象conn，表名bridge_name,节点名node_name,查询日期date,质量分析结果result,表名table_name\n",
    "def insert_into_qual_pie(conn,bridge_name,node_name,date,result,table_name = \"qual_pie_report\"):\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        insert_sql = \"insert into \" + table_name + \"(bridge_name,bridge_id,node_name,date_inquery,data_num,execent_num,good_num,soso_num,\\\n",
    "                bad_num,worse_num)VALUES(\\'%s\\',\\'%s\\',\\'%s\\',\\'%s\\',\\'%s\\',\\'%s\\',\\'%s\\',\\'%s\\',\\'%s\\',\\'%s\\')\"\\\n",
    "                %(bridge_name,dictionary02.get(bridge_name),node_name,date,sum(result),result[0],result[1],result[2],result[3],result[4])\n",
    "        cur.execute(insert_sql)\n",
    "        cur.close()\n",
    "        conn.commit()\n",
    "    except:\n",
    "        print(\"插入质量分析饼图绘图数据失败！\")\n",
    "        return \"wrong\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  创建傅里叶变换数据表\n",
    "##  输入：数据库操作对象 conn，表名 table_name\n",
    "def create_fft_table(conn,table_name):\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        create_sql = \"create table \" + table_name + \"(ID INT(100) not null AUTO_INCREMENT comment '记录自增主键',fft_freq FLOAT(10,6) comment '频率',\\\n",
    "                fft_value FLOAT(10,4) comment '傅里叶变换值',Package_Number INT unsigned not null comment '所属包编号',primary key(ID))ENGINE = InnoDB DEFAULT CHARSET=UTF8MB4;\"\n",
    "        cur.execute(create_sql)\n",
    "        cur.close()\n",
    "        conn.commit()\n",
    "    except:\n",
    "        print(\"创建傅里叶变换数据表失败！\")\n",
    "        return \"wrong\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  向傅里叶变换数据表插入数据\n",
    "##  输入：数据库操作对象 conn，表名 table_name，数据 fft_time，数据开始记录时间 begin_date，采样频率 freq，包名 package_name\n",
    "def insert_into_fft_table(conn,table_name,fft_data,freq,package_name,):\n",
    "    try:\n",
    "        i = 0\n",
    "        # 横坐标变换为频率\n",
    "        freq = (1000000 / freq) / 2\n",
    "        x = np.arange(0,len(fft_data)) / len(fft_data) * freq\n",
    "        cur = conn.cursor()\n",
    "        insert_sql = \"INSERT INTO \" + table_name + \" (fft_freq,fft_value,Package_Number) VALUES \"\n",
    "        for d in fft_data[0:-1]:\n",
    "            insert_data = \"(\\'\" + str(x[i]) + \"\\',\" + str(d) + \",\" + str(package_name) + \"),\"\n",
    "            insert_sql += insert_data\n",
    "            i += 1\n",
    "        insert_sql += \"(\\'\" + str(x[i]) + \"\\',\" + str(fft_data[len(fft_data)-1]) +\",\" + str(package_name) +\");\"\n",
    "        n = cur.execute(insert_sql)\n",
    "        cur.close()\n",
    "        conn.commit()\n",
    "    except:\n",
    "        print(\"插入傅里叶变换数据失败！\")\n",
    "        return \"wrong\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  创建地址文件 \n",
    "##  输入：目录地址\n",
    "##  创建成功返回true 已存在返回false\n",
    "def mkdir_ifnotExist(path):\n",
    "    try:\n",
    "        path = path.strip() #删除地址首尾空格\n",
    "        path = path.rstrip(\"\\\\\") #保留\\\\之后的内容 以空格结尾\n",
    "        isExist = os.path.exists(path)\n",
    "        if not isExist :\n",
    "            os.makedirs(path)\n",
    "            #print(\"%s did not exist.\\nNow is created.\"%path)\n",
    "            return True\n",
    "        else :\n",
    "            #print(\"%s has exist!\"%path)\n",
    "            return False\n",
    "    except:\n",
    "        print(\"创建地址路径失败！\")\n",
    "        return \"wrong\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  移动文件 \n",
    "##  假设拷贝的文件路径为 盘/文件1/文件2/.../桥名/年/月/日/检测类型/数据文件名（这个就是source_path）\n",
    "##  复制到的路径为 你指定的文件夹（这个是direct_path）/桥名/年/月/日/检测类型/数据文件名 \n",
    "##  输入：源文件绝对路径source_path，源文件的本地存放目录direct_path\n",
    "##  输出：成功 True；源文件不存在 False\n",
    "def move_file(source_path , direct_path):\n",
    "    try:\n",
    "        if os.path.exists(source_path):\n",
    "            [cwd_path,filename] = os.path.split(direct_path)\n",
    "            mkdir_ifnotExist(direct_path)\n",
    "            shutil.copy(source_path , direct_path)\n",
    "            #print(\"move %s -> %s\"%(source_path , direct_path))\n",
    "            return True\n",
    "        else :\n",
    "            print(\"files not exists\")\n",
    "            return False\n",
    "    except:\n",
    "        print(\"移动文件失败！\")\n",
    "        return \"wrong\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  从消息队列获取文件消息，读取、存放\n",
    "##  输入：7z文件转换txt所需的参数parameter（C:/Users/Administrator/Desktop/SMJ_DYB-03-01_010000_013402.DY.7z）即传入文件的绝对路径\n",
    "##        解压缩程序绝对路径command\n",
    "##  输出：txt文件名及其绝对路径\n",
    "def file_get_from_message(parameter,command = \"C:/Users/Administrator/Desktop/数据转换小程序/DataTransform.exe\"):\n",
    "    try:\n",
    "        # command是转换程序exe的绝对路径\n",
    "        # 首先要进入到转换程序所在目录\n",
    "        command_dir = command.split(\"/\")\n",
    "        enter_dir = \"\"\n",
    "        for c_dir in command_dir[0:-1]:\n",
    "            enter_dir += c_dir + \"/\"\n",
    "        os.chdir(enter_dir)\n",
    "        #os.chdir(\"C:/Users/Administrator/Desktop/数据转换小程序\")\n",
    "        file_name = parameter.split(\".\")[0]    # 相对路径 + SMJ_DYB-03-01_010000_013402 （.../SMJ_DYB-03-01_010000_013402）\n",
    "        suffix = parameter.split(\".\")[1]       # 压缩词缀、传感器类型 DY\n",
    "        parameter1 = file_name + \".\" + suffix  # 第二条指令的参数1：第一条命令执行后产生文件的绝对路径 .../SMJ_DYB-03-01_010000_013402.DY\n",
    "        parameter2 = file_name + \".txt\"        # 第二条指令的参数2：数据存放的文件的绝对路径 .../SMJ_DYB-03-01_010000_013402.txt\n",
    "        # 执行转换程序（参数加上目录组成绝对路径）\n",
    "        state1 = os.popen(command + \" \" + parameter + \" \" + parameter).read()\n",
    "        print(\"解压文件第一步完成:\",state1)\n",
    "        state2 = os.popen(command + \" \" + parameter1 + \" \" + parameter2).read()\n",
    "        print(\"解压文件第二步完成:\",state2)\n",
    "        # 返回文件名SMJ_DYB-03-01_010000_013402.txt和txt绝对路径\n",
    "        return parameter2.split(\"/\")[-1],parameter2\n",
    "    except:\n",
    "        print(\"解压文件失败！\")\n",
    "        return \"wrong\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  将txt放置对应目录下\n",
    "##  输入：文件名file_name（SMJ_DYB-03-01_010000_013402.txt），文件txt的绝对路径source_path，保存根目录save_path（可作默认值）\n",
    "##  输出：该txt具体保存在本地的目录\n",
    "def put_file_in(file_name,source_path,save_path = \"C:/Users/Administrator/Desktop/桥梁备份数据\"):\n",
    "    try:\n",
    "        # 文件信息拆分\n",
    "        file_info = file_name.split(\"_\")\n",
    "        bridge_name = dictionary03.get(file_info[0])\n",
    "        node_name = file_info[1]\n",
    "        begin_time = file_info[2]\n",
    "        end_time = file_info[3].split(\".\")[0]\n",
    "        # 获取当日日期\n",
    "        year = str(datetime.datetime.now().year)\n",
    "        month = str(datetime.datetime.now().month)\n",
    "        day = str(datetime.datetime.now().day)\n",
    "        # 对文件类型进行判断\n",
    "        # txt为传感器数据、doc为其他等\n",
    "        if file_info[3].split(\".\")[1] == \"txt\":  \n",
    "            save_path = os.path.join(save_path,bridge_name,\"传感器数据\",year,month,day,node_name)\n",
    "        else:\n",
    "            pass\n",
    "        # 检查该文件保存目录是否存在，否则建之\n",
    "        mkdir_ifnotExist(save_path)\n",
    "        # 移动该文件至指定目录\n",
    "        move_file(source_path,save_path)\n",
    "        # 返回该文件当前所在目录\n",
    "        return save_path\n",
    "    except:\n",
    "        print(\"剪切文件置对应目录失败！\")\n",
    "        return \"wrong\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  获取指定时间段内数据\n",
    "##  输入：桥梁名bridgename，节点detectpoint，起始日期date_begin，终止日期date_end，数据存放根目录local_path\n",
    "##  输出：DataFrame结构数据raws\n",
    "def get_specified_data(bridgename,detectpoint,date_begin,date_end,local_path = \"C:/Users/Administrator/Desktop/桥梁备份数据\"):\n",
    "    try:\n",
    "        # 将要分析的日期格式化\n",
    "        date1 = datetime.datetime.strptime(date_begin,\"%Y-%m-%d\")\n",
    "        date2 = datetime.datetime.strptime(date_end,\"%Y-%m-%d\")\n",
    "        # 根据01字典获取桥梁在文件中的名字\n",
    "        bridge_code = dictionary01.get(bridgename)\n",
    "        # 录入数据库中的时间数据\n",
    "        input_date = str(date1)[0:10] + \"--\" + str(date2)[0:10]\n",
    "        # TMP————文件名与ftp路径\n",
    "        tmp_filename = bridge_code + \"_\" + detectpoint + \"_\"\n",
    "        tmp_path = os.path.join(local_path,bridge_code,\"传感器数据\")\n",
    "        # 利用date进行查询日期范围内的迭代 \n",
    "        date = date1\n",
    "        first_file_flag = 1\n",
    "        while date <= date2:\n",
    "            # 扩展ftp路径\n",
    "            source_path = os.path.join(tmp_path,str(date.year),str(date.month),str(date.day),detectpoint)\n",
    "            '''\n",
    "            # 将日期转换为str格式\n",
    "            year = str(date.year)\n",
    "            # 将“月”与“日”拓展为“XX”形式\n",
    "            if date.month in number:\n",
    "                month = \"0\" + str(date.month)\n",
    "            if date.day in number:\n",
    "                day = \"0\" + str(date.day)\n",
    "            '''\n",
    "            # 遍历指定目录下所有文件\n",
    "            for root, dirs, files in os.walk(source_path):\n",
    "                for file in files: \n",
    "                    # 合成最终读取文件的完整地址 读并存入raws中\n",
    "                    file = os.path.join(source_path,file).replace(\"\\\\\",\"/\")\n",
    "                    print(file)\n",
    "                    if( date == date1 and first_file_flag == 1 ):\n",
    "                        raws = pd.read_csv(file,header = 1,names = [\"value\",\"remarks\"],engine='python')\n",
    "                        first_file_flag == 0\n",
    "                    else :\n",
    "                        tmp = pd.read_csv(file,header = 1,names = [\"value\",\"remarks\"],engine='python')\n",
    "                        raws = pd.concat([raws,tmp],axis=0,ignore_index=True)\n",
    "            # 日期 + 1\n",
    "            date = date + datetime.timedelta(days=1)    \n",
    "\n",
    "        # raws存储日期范围内的数据 DataFrame结构\n",
    "        return raws\n",
    "    except:\n",
    "        print(\"获取指定时段内数据失败！\")\n",
    "        return \"wrong\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  样条插值函数\n",
    "##  输入：原始数据 primitive_values\n",
    "##  输出：样条插值处理后的数据\n",
    "def insert_value(primitive_values):\n",
    "    try:\n",
    "        a = []#空值的序号\n",
    "        b = []#空值的值\n",
    "        c = []#非空值的序号\n",
    "        d = []#非空值的值\n",
    "        # 找空值\n",
    "        for i in range(len(primitive_values)):\n",
    "            if math.isnan(primitive_values[i]):\n",
    "                a.append(i)\n",
    "                #b.append(primitive_values[i])\n",
    "            else:\n",
    "                c.append(i)\n",
    "                d.append(primitive_values[i])\n",
    "        s = pd.Series(d , index = c)\n",
    "        s = s.sort_values(ascending = True)\n",
    "        # 建模\n",
    "        func = interpolate.interp1d(c,d,kind='cubic')\n",
    "        # 应插入的值\n",
    "        b = func(a)\n",
    "        # 补充空值（插值）\n",
    "        for i in range(len(a)):\n",
    "            primitive_values[a[i]] = b[i]\n",
    "        # 返回插值后的结果\n",
    "        return primitive_values\n",
    "    except:\n",
    "        print(\"样条插值失败，以均值插入！\")\n",
    "        return primitive_values.fillna(primitive_values.mean())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  平滑数据\n",
    "##  输入：需平滑的数据data（Series）\n",
    "##  输出：平滑后结果result（Series）\n",
    "def filtering_data(data):\n",
    "    try:\n",
    "        result = pd.Series(savgol_filter(data,5,2))\n",
    "        return result\n",
    "    except:\n",
    "        print(\"平滑数据失败！\")\n",
    "        return \"wrong\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  统计分析\n",
    "##  输入：需分析的数据data（Series）\n",
    "##  输出：统计结果（Series）\n",
    "def statistics_analyze(data):\n",
    "    try:\n",
    "        return pd.to_numeric(data,errors='ignore').describe()\n",
    "    except:\n",
    "        print(\"统计分析失败！\")\n",
    "        return \"wrong\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  值分析\n",
    "##  输入：需要分析的数据df（Series）\n",
    "##  输出：分析结果（list）\n",
    "def value_analyze(df):\n",
    "    try:\n",
    "        # 数量统计\n",
    "        null_num = df[\"value\"].isnull().sum()\n",
    "        not_null_df = df['value'].count()\n",
    "        positive_df = df[df['value'] > 0].count()['value']\n",
    "        negative_df = df[df['value'] < 0].count()['value']\n",
    "        normal_data = df[(df['value'] >= df['value'].describe()[1] - 3 * df['value'].describe()[2]) & (df['value'] <= df['value'].describe()[1] + 3 * df['value'].describe()[2])]\n",
    "        unnormal_count = not_null_df - normal_data['value'].count()\n",
    "        # 比重统计\n",
    "        null_percentage = null_num / (not_null_df + null_num)\n",
    "        positive_percentage = positive_df / (not_null_df + null_num)\n",
    "        negative_percentage = negative_df / (not_null_df + null_num)\n",
    "        unnormal_percentage = unnormal_count / (not_null_df + null_num)\n",
    "        # 极差、众数、偏度\n",
    "        max_value = df['value'].max()\n",
    "        min_value = df['value'].min()\n",
    "        jicha = max_value - min_value\n",
    "        zhongshu = df['value'].mode()[0]\n",
    "        skew = df['value'].skew()\n",
    "        # 整体数据质量\n",
    "        overall_ineffect = null_percentage + unnormal_percentage + (positive_percentage if positive_percentage < negative_percentage else negative_percentage)\n",
    "        # 数据分布占比\n",
    "        interval = (max_value - min_value)/4\n",
    "        value_quarter = min_value + interval\n",
    "        value_half = min_value + 2*interval\n",
    "        value_three_quarter = min_value + 3*interval\n",
    "        # 各分布块所占比例\n",
    "        a_df = df[(df['value'] >= min_value)&(df['value'] < value_quarter)]\n",
    "        a_percentage = a_df['value'].count() / df['value'].count()\n",
    "        b_df = df[ (df['value'] >= value_quarter)&(df['value'] < value_half)]\n",
    "        b_percentage = b_df['value'].count() / df['value'].count()\n",
    "        c_df = df[(df['value'] >= value_half)&(df['value'] < value_three_quarter)]\n",
    "        c_percentage = c_df['value'].count() / df['value'].count()\n",
    "        d_df = df[(df['value'] >= value_three_quarter)&(df['value'] <= max_value)]\n",
    "        d_percentage = d_df['value'].count() / df['value'].count()\n",
    "        # 返回分析结果列表\n",
    "        return [not_null_df,positive_df,negative_df,positive_percentage,negative_percentage,unnormal_count,unnormal_percentage,\\\n",
    "               null_percentage,jicha,zhongshu,skew,overall_ineffect,a_percentage,b_percentage,c_percentage,d_percentage,null_num,(not_null_df + null_num)]\n",
    "    except:\n",
    "        print(\"值分析失败！\")\n",
    "        return \"wrong\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  直方图分析 取整\n",
    "##  输入：数据集df（Series），分组数group_num\n",
    "##  输出：直方图横坐标的数据列表hist_x，直方图横坐标对应纵坐标的数据列表hist_y\n",
    "def demo_hist_dist_analyse(df , group_num):\n",
    "    try:\n",
    "        # 取极值\n",
    "        min_floor = math.floor(df['value'].min())\n",
    "        max_ceil = math.ceil(df['value'].max())\n",
    "        # 极差\n",
    "        delta = ( max_ceil - min_floor ) / group_num\n",
    "        # 储值列表\n",
    "        hist_count = list()    # 该块数量（y）\n",
    "        hist_value = list()    # 该块坐标（x）\n",
    "        hist_value.append(min_floor)\n",
    "        i = 1    # 记序\n",
    "        # 储值列表赋初值\n",
    "        while i <= group_num:\n",
    "            hist_count.append(0)\n",
    "            hist_value.append(min_floor+i*delta)\n",
    "            i+=1\n",
    "        # 统计\n",
    "        for tmp_value in df['value']:\n",
    "            hist_count[ int((math.floor(tmp_value) - min_floor) / delta) ] += 1\n",
    "        # 返回结果\n",
    "        return hist_value,hist_count\n",
    "    except:\n",
    "        print(\"直方图分析失败！\")\n",
    "        return \"wrong\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  数据分布直方图分析\n",
    "##  输入：数据集df（Series），分组数group_num\n",
    "##  输出：直方图横坐标的数据列表hist_x，直方图横坐标对应纵坐标的数据列表hist_y\n",
    "def hist_dist_analyse(df , group_num):\n",
    "    try:\n",
    "        # 取极值\n",
    "        min_floor = (df['value'].min())\n",
    "        max_ceil =(df['value'].max())\n",
    "        # 极差\n",
    "        delta = ( max_ceil - min_floor ) * 1.001 / group_num    # 考虑到最大值问题 乘上1.001系数\n",
    "        # 储值列表\n",
    "        hist_count = list()    # 该块数量（y）\n",
    "        hist_value = list()    # 该块坐标（x）\n",
    "        hist_value.append(round(min_floor,4))\n",
    "        i = 1    # 记序\n",
    "        # 储值列表赋初值\n",
    "        while i <= group_num:\n",
    "            hist_count.append(0)\n",
    "            hist_value.append(round((min_floor+i*delta),4))\n",
    "            i+=1\n",
    "        # 统计\n",
    "        for tmp_value in df['value']:\n",
    "            hist_count[ math.floor((tmp_value - min_floor) / delta) ] += 1\n",
    "        # 返回结果\n",
    "        return hist_value,hist_count\n",
    "    except:\n",
    "        print(\"数据分布直方图分析失败！\")\n",
    "        return \"wrong\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  数据分布饼图\n",
    "##  输入：数据集df（Series）\n",
    "##  输出：饼图四块的占比fraces，对应范围标签labels\n",
    "def distribute_pie_analyse(df):\n",
    "    try:\n",
    "        # 数据分布饼图    \n",
    "        max_value = df['value'].max()\n",
    "        min_value = df['value'].min()\n",
    "        interval = (max_value - min_value)/4\n",
    "        value_quarter = min_value + interval\n",
    "        value_half = min_value + 2*interval\n",
    "        value_three_quarter = min_value + 3*interval\n",
    "\n",
    "        # 传入数据库中的各块标签\n",
    "        labels = ['A:{:.1f}-{:.1f}'.format(min_value,value_quarter),'B:{:.1f}-{:.1f}'.format(value_quarter,value_half),'C:{:.1f}-{:.1f}'.format(value_half,value_three_quarter),'D:{:.1f}-{:.1f}'.format(value_three_quarter,max_value)]\n",
    "\n",
    "        # 各数据块所占比例\n",
    "        a_df = df[(df['value'] >= min_value)&(df['value'] < value_quarter)]\n",
    "        a_percentage = a_df['value'].count() / df['value'].count()\n",
    "        b_df = df[ (df['value'] >= value_quarter)&(df['value'] < value_half)]\n",
    "        b_percentage = b_df['value'].count() / df['value'].count()\n",
    "        c_df = df[(df['value'] >= value_half)&(df['value'] < value_three_quarter)]\n",
    "        c_percentage = c_df['value'].count() / df['value'].count()\n",
    "        d_df = df[(df['value'] >= value_three_quarter)&(df['value'] <= max_value)]\n",
    "        d_percentage = d_df['value'].count() / df['value'].count()\n",
    "        # 传入数据库中的各块占比\n",
    "        fraces = [a_percentage,b_percentage,c_percentage,d_percentage]\n",
    "        return fraces,labels\n",
    "    except:\n",
    "        print(\"数据分布饼图分析失败！\")\n",
    "        return \"wrong\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  数据质量分析饼图\n",
    "##  输入：数据df、数据均值total_mean、数据标准差total_std，等级划分level_num默认为5\n",
    "##  输出：各等级数据数\n",
    "def quality_pie_analyse(df,total_mean,total_std,level_num = 5):\n",
    "    try:\n",
    "        # 根据等级划分指标和评价  未做  默认4级\n",
    "        execent_count = 0    # 优秀数据数\n",
    "        good_count = 0       # 良好数据数\n",
    "        soso_count = 0       # 中等数据数\n",
    "        bad_count = 0        # 差数据数\n",
    "        worse_count = 0      # 极差数据数\n",
    "        # 结果列表\n",
    "        result_list = list()\n",
    "        # 绘制饼图的数据，呈现数据块占比\n",
    "        for tmp_value in df['value']:\n",
    "            # 根据公式进行数据值质量判定\n",
    "            tmp_level = (tmp_value - total_mean ) / total_std\n",
    "            if tmp_level < 0.2 :  \n",
    "                execent_count += 1\n",
    "            elif tmp_level < 0.4 :\n",
    "                good_count += 1\n",
    "            elif tmp_level < 0.5 :\n",
    "                soso_count += 1\n",
    "            elif tmp_level < 0.7 :\n",
    "                bad_count += 1\n",
    "            else:\n",
    "                worse_count += 1 \n",
    "        # 将结果保存进输出列表\n",
    "        result_list.append(execent_count)\n",
    "        result_list.append(good_count)\n",
    "        result_list.append(soso_count)\n",
    "        result_list.append(bad_count)\n",
    "        result_list.append(worse_count)\n",
    "        # 输出结果\n",
    "        return result_list\n",
    "    except:\n",
    "        print(\"数据质量饼图分析失败！\")\n",
    "        return \"wrong\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  将数据报中的空数据置空\n",
    "##  输入：原始数据（Series）Original_value，空数据标志 need_replace_value\n",
    "def set_null_value(Original_value,need_replace_value):\n",
    "    try:\n",
    "        new_value = Original_value.replace(need_replace_value, float(\"nan\"))\n",
    "        return new_value\n",
    "    except:\n",
    "        print(\"空数据处理失败！\")\n",
    "        return \"wrong\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  删除质量极差数据\n",
    "##  输入：数据df，均值total_mean，标准差total_std，阈值threshold\n",
    "##  输出：删除后的数据（删除的用'nan'代替），修改的数量\n",
    "def delte_bad_data(df,total_mean,total_std,threshold = 0.8):\n",
    "    try:\n",
    "        nan_record = df['value'].isna()\n",
    "        for i in range(0,len(df['value'])):\n",
    "            if (nan_record[i]) == False and (((df['value'].loc[i] - total_mean) / total_std ) >= threshold) :\n",
    "                df['value'].at[i] = np.nan\n",
    "        return df\n",
    "    except:\n",
    "        print(\"删除质量极差数据失败！\")\n",
    "        return \"wrong\"\n",
    "    '''\n",
    "    index = 0\n",
    "    count = 0\n",
    "    for i_value in df['value']:\n",
    "        # 根据公式进行数据值质量判定\n",
    "        if (( i_value - total_mean ) / total_std ) >= threshold :\n",
    "            df[\"value\"][index] = np.nan\n",
    "            count += 1\n",
    "        index += 1\n",
    "        print(\"删除质量极差数据后：\",df['value'])\n",
    "    return df , count\n",
    "    '''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  傅里叶变换\n",
    "##  输入：原始数据 data，采样频率 fre = 100\n",
    "##  输出：变换后各采样点频率 \n",
    "def fft_transform(data,fre = 100):\n",
    "    try:\n",
    "        fft_data = fft(data)\n",
    "        fft_data = abs(fft_data)\n",
    "        fft_data[0] = 0\n",
    "        return fft_data\n",
    "    except:\n",
    "        print(\"数据傅里叶变换失败！\")\n",
    "        return \"wrong\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  预处理+质量分析操作\n",
    "##  输入：txt文件绝对路径source_path，文件名file_name\n",
    "def read_analyze_save_file(source_path , file_name , package_number):\n",
    "    try:\n",
    "        # 将字母小写\n",
    "        file_info = file_name.lower().split(\"_\")\n",
    "        # 表名类：桥_节点\n",
    "        table_name = file_info[0] + \"_\" + file_info[1].replace(\"-\",\"_\")\n",
    "        # 两个拓展表名\n",
    "        table_name_quality_report = table_name + \"_quality_report\"\n",
    "        table_name_alter = table_name + \"_data\"\n",
    "        # 检测类型\n",
    "        detect_name = file_info[1].split(\"-\")[0]\n",
    "        # 起始时间 str\n",
    "        begin_time = datetime.datetime.now().strftime(\"%Y-%m-%d\") + \" \" \\\n",
    "                    + file_info[2][0:2] + \":\" + file_info[2][2:4] + \":\" + file_info[2][4:6] + \".0\"\n",
    "        # 起始时间 datetime：%Y-%m-%d %H:%M:%S.%f\n",
    "        begin_time = datetime.datetime.strptime(begin_time,\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "        # 读取文件路径\n",
    "        file_source = os.path.join(source_path,file_name).replace(\"\\\\\",\"/\")\n",
    "        # 文件中的数据 remarks为none\n",
    "        file_data = pd.read_csv(file_source,header = 1,names = [\"value\",\"remarks\"],engine='python')\n",
    "        #print(\"原始数据：\",file_data[\"value\"])\n",
    "        # 判断是否丢包\n",
    "        # 丢包\n",
    "        if (file_data.shape[0] == 0):\n",
    "            print(\"该包信息丢失！丢包信息请参看lost_package_table！\",table_name)\n",
    "            conn = connect_mysql(host,user,password)\n",
    "            # 创建丢包记录表\n",
    "            if table_exist(conn,\"lost_package_table\") is not True:\n",
    "                create_lost_package_table(conn,\"lost_package_table\")\n",
    "            # 查询该节点先前的丢包信息\n",
    "            package_last_lost_info = inquire_lost_info(conn,table_name)\n",
    "            # 如果该节点第一次丢失，则初始化该节点记录\n",
    "            if package_last_lost_info == () :\n",
    "                insert_into_lost_table(conn,table_name,1,0)\n",
    "            # 已存在丢包记录\n",
    "            else :\n",
    "                # 获取该节点之前的丢包记录\n",
    "                last_lost_num = package_last_lost_info[0][3]\n",
    "                last_lost_flag = package_last_lost_info[0][4]\n",
    "                last_lost_time = package_last_lost_info[0][2]\n",
    "                # 计算前后丢包事件差\n",
    "                delta = datetime.datetime.now() - last_lost_time\n",
    "                # 判断续包次数\n",
    "                delta_h = delta.total_seconds() // 10.0\n",
    "                # 如果续包小于1 即连续丢包\n",
    "                if delta_h < 1.0 :\n",
    "                    # 丢包数+1\n",
    "                    lost_num = last_lost_num + 1\n",
    "                    # 如果丢包数大于等于3且报警位不是2（已知，取消报警信息，继续计数）\n",
    "                    if lost_num >= 3 and last_lost_flag != 2:\n",
    "                        # 令报警位置1（发出报警信息）\n",
    "                        lost_flag = 1\n",
    "                    else:\n",
    "                        # 保持报警位0（无需报警）\n",
    "                        lost_flag = last_lost_flag\n",
    "                # 如果有续包\n",
    "                else :\n",
    "                    # 如果续包数大于丢包数+1，则丢包记录清空\n",
    "                    if last_lost_num - delta_h <= -1.0:\n",
    "                        lost_num = 0\n",
    "                        lost_flag = 0\n",
    "                    # 否则 累计丢包数 上一次丢包数 - 中间续包数 + 1（本次丢包）\n",
    "                    else:\n",
    "                        lost_num = last_lost_num - delta_h + 1\n",
    "                        # 如果累计后丢包数小于3 报警位值0\n",
    "                        if lost_num < 3:\n",
    "                            lost_flag = 0\n",
    "                        # 否则 报警位保留\n",
    "                        else:\n",
    "                            lost_flag = last_lost_flag\n",
    "                # 更新该节点丢包记录\n",
    "                update_lost_table(conn,table_name,lost_num,lost_flag)\n",
    "            conn.close()\n",
    "        # 未丢包\n",
    "        else:\n",
    "            # 将原始数据取出\n",
    "            data_value = file_data[\"value\"]\n",
    "            # 连接数据库\n",
    "            conn = connect_mysql(host,user,password)\n",
    "            '''\n",
    "            # 判断该节点源数据表是否存在\n",
    "            if table_exist(conn,table_name_original) is not True:\n",
    "                create_data_table(conn,table_name_original)\n",
    "            # 在对应表中顺序插入最新数据\n",
    "            insert_into_original_table(conn,table_name_original,data_value,begin_time,FREQ_REFERENCE.get(detect_name),package_number)\n",
    "            '''\n",
    "            # 将包中丢失的数据置 NaN\n",
    "            data_value = set_null_value(data_value,dictionary04.get(detect_name))\n",
    "            #print(\"置空后的data_value:\",data_value)\n",
    "            file_data = pd.DataFrame(data_value,columns=[\"value\"])\n",
    "            #print(\"置空后的file_data：\",file_data)\n",
    "            # 创建质量分析报告表\n",
    "            if table_exist(conn,table_name_quality_report) is not True:\n",
    "                create_report_table(conn,table_name_quality_report)\n",
    "            # 统计分析\n",
    "            sta_analyse = statistics_analyze(data_value)\n",
    "            # 值分析\n",
    "            value_analyse = value_analyze(file_data)\n",
    "            # 向报告表中插入最近包的质量分析报告\n",
    "            insert_into_reporttable(conn,file_info[0],file_info[1],begin_time,sta_analyse,value_analyse,package_number,table_name_quality_report)\n",
    "            # 删除质量差的数据\n",
    "            data_after_delte = delte_bad_data(file_data,sta_analyse[1],sta_analyse[2])\n",
    "            #print(\"删除质量差的数据后：\",data_after_delte)\n",
    "            #data_after_delte ,delte_num = delte_bad_data(file_data,sta_analyse[1],sta_analyse[2])\n",
    "            # 3阶B样条曲线插值\n",
    "            data_after_insert = insert_value(data_after_delte[\"value\"])\n",
    "            #print(\"插值后的数据：\",data_after_insert)\n",
    "            # 平滑处理\n",
    "            data_after_filtering = filtering_data(data_after_insert)\n",
    "            #print(\"平滑后的数据：\",data_after_filtering)\n",
    "            # 查询是否创建该节点预处理后的数据表\n",
    "            if table_exist(conn,table_name_alter) is not True:\n",
    "                create_alterdata_table(conn,table_name_alter)\n",
    "            # 将预处理后的数据存入数据库\n",
    "            insert_into_alter_table(conn,table_name_alter,data_after_filtering,begin_time,FREQ_REFERENCE.get(detect_name),package_number)\n",
    "            # 对于振动传感器数据进行傅里叶变换\n",
    "            if detect_name == \"jsd\":\n",
    "                table_name_fft = table_name + \"_fft\"\n",
    "                fft_draw = draw_fft_thread(table_name_fft,detect_name,begin_time,data_after_filtering)\n",
    "                fft_draw.start()\n",
    "            # 关闭数据库连接\n",
    "            close_mysql(conn)\n",
    "            \n",
    "            return table_name_quality_report,file_info[0],file_info[1].replace(\"-\",\"_\"),data_after_filtering,package_number\n",
    "        \n",
    "    except:\n",
    "        print(\"数据质量分析 & 预处理失败！\")\n",
    "        return \"wrong\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  绘图程序\n",
    "##  输入：所选桥梁名bridgename，节点名detectpoint，起始日期date_begin，终止日期date_end，数据文件存放根目录local_path，柱状图参数rects\n",
    "def draw_quality_map(bridgename,detectpoint,date_begin,date_end,local_path = \"C:/Users/Administrator/Desktop/桥梁备份数据\",rects=8):\n",
    "    try:\n",
    "        df = get_specified_data(bridgename,detectpoint,date_begin,date_end)\n",
    "        # 将传感器值数值化\n",
    "        df['value'] = pd.to_numeric(df['value'],errors='ignore')\n",
    "        # 得到统计数据\n",
    "        file_info = df['value'].describe()\n",
    "        # 给存储日期做判断\n",
    "        if date_begin != date_end :\n",
    "            date_begin = date_begin.split(\"-\")[0] + \"-\" + date_begin.split(\"-\")[1]\n",
    "        # 连接数据库\n",
    "        conn = connect_mysql(host,user,password)\n",
    "        # 统计直方图数据表（只1次）\n",
    "        if table_exist(conn,\"stat_hist_report\") is not True:\n",
    "            create_stat_hist_table(conn,\"stat_hist_report\")\n",
    "        # 绘图数据插入数据表\n",
    "        insert_into_stat_hist(conn,bridgename,detectpoint,date_begin,df,file_info)\n",
    "        # 统计直方图绘制数据表（只1次）\n",
    "        if table_exist(conn,\"dist_hist_report\") is not True:\n",
    "            create_dist_hist_table(conn,\"dist_hist_report\")\n",
    "        # 直方图分析\n",
    "        hist_x , hist_y = hist_dist_analyse(df,rects)\n",
    "        # 绘图数据插入数据表\n",
    "        insert_into_dist_hist(conn,bridgename,detectpoint,date_begin,hist_x,hist_y)\n",
    "        # 质量分析饼图数据表（只1次）\n",
    "        if table_exist(conn,\"qual_pie_report\") is not True:\n",
    "            create_qual_pie_table(conn,\"qual_pie_report\")\n",
    "        # 质量分析饼图分析\n",
    "        quality_result = quality_pie_analyse(df,file_info[1],file_info[2])\n",
    "        # 绘图数据插入数据表\n",
    "        insert_into_qual_pie(conn,bridgename,detectpoint,date_begin,quality_result)\n",
    "        '''\n",
    "        # 数据分布饼图数据表（只1次）\n",
    "        if table_exist(conn,\"dist_pie_report\") is not True:\n",
    "            create_dist_pie_table(conn,\"dist_pie_report\")\n",
    "        # 数据分布饼图分析\n",
    "        pie_perc,pie_lab = distribute_pie_analyse(df)\n",
    "        # 绘图数据插入数据表\n",
    "        insert_into_dist_pie(conn,bridgename,detectpoint,date_begin,pie_perc,pie_lab)\n",
    "        '''\n",
    "        close_mysql(conn)\n",
    "    except:\n",
    "        print(\"绘图失败！\")\n",
    "        return \"wrong\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  主函数 程序逻辑 后台一直运作\n",
    "def background_running(file_path_name):\n",
    "    #try:\n",
    "        # 包标识号+1\n",
    "        global package_number\n",
    "        package_number += randint(1,99999)\n",
    "        # 从消息机制获取7z转换为txt\n",
    "        #detect_file_name ,read_path = file_get_from_message(file_path_name)\n",
    "        detect_file_name = file_path_name.split(\"/\")[-1]\n",
    "        # 将txt保存到本地树型目录\n",
    "        tree_path = put_file_in(detect_file_name,file_path_name)\n",
    "        # 分析目录下指定txt数据，并存入mysql两表\n",
    "        table_name,bridge_name,node_name,data_after_filtering,package_name = read_analyze_save_file(tree_path,detect_file_name,package_number)\n",
    "        # 异常检测模块\n",
    "        ansn(table_name,bridge_name,node_name,data_after_filtering,package_name)\n",
    "    #except:\n",
    "    #    print(\"数据处理主程序运行出错！\")\n",
    "    #    return \"wrong\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  根据下发模块加载节点信息\n",
    "##  输入：数据库基本信息 my_host,my_user,my_password,my_database，下发表名 table_name\n",
    "##  输出：节点类列表 node_info_list\n",
    "def load_node_info(my_host,my_user,my_password,_my_database,table_name):\n",
    "    try:\n",
    "        conn = connect_mysql(my_host,my_user,my_password,my_database = \"testt\",my_port = 3306,my_charset = \"utf8\")\n",
    "        cur = conn.cursor()\n",
    "        sql = \"select * from \" + table_name + \";\"\n",
    "        cur.execute(sql)\n",
    "        results = cur.fetchall()\n",
    "        node_info_list = []\n",
    "        row_num = 0\n",
    "        for row_data in results:\n",
    "            node_info_list.append(\"\")\n",
    "            node_info_list[row_num] = BridgeNode_Info(row_data)\n",
    "            row_num += 1\n",
    "        cur.close()\n",
    "        conn.commit()\n",
    "        close_mysql(conn)\n",
    "        return node_info_list\n",
    "    except:\n",
    "        print(\"读取下发模块节点信息失败！\")\n",
    "        return \"wrong\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  每日质量分析绘图线程\n",
    "class draw_daily_report_thread(threading.Thread):   #继承父类threading.Thread\n",
    "    def __init__(self,bridgename,notedname,date1,date2,rects):\n",
    "        try:\n",
    "            threading.Thread.__init__(self)\n",
    "            self.bridgename = bridgename\n",
    "            self.notename = notedname\n",
    "            self.date1 = date1\n",
    "            self.date2 = date2\n",
    "            self.rects = rects\n",
    "        except:\n",
    "            print(\"每日质量分析绘图线程初始化失败！\")\n",
    "            return \"wrong\"\n",
    "        \n",
    "    def run(self):                   #把要执行的代码写到run函数里面 线程在创建后会直接运行run函数 \n",
    "        print(\"Start drawing yesterday's report :\",self.bridgename,\"-\",self.notename,\"-\",self.date1,\"-\")\n",
    "        draw_quality_map(self.bridgename,self.notename,self.date1,self.date2,self.rects)\n",
    "        print(\"Drawing down !\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  每月质量分析绘图线程\n",
    "class draw_monthly_report_thread(threading.Thread):   #继承父类threading.Thread\n",
    "    def __init__(self,bridgename,notedname,date1,date2,rects):\n",
    "        try:\n",
    "            threading.Thread.__init__(self)\n",
    "            self.bridgename = bridgename\n",
    "            self.notename = notedname\n",
    "            self.date1 = date1\n",
    "            self.date2 = date2\n",
    "            self.rects = rects\n",
    "        except:\n",
    "            print(\"每月质量分析绘图线程初始化失败！\")\n",
    "            return \"wrong\"\n",
    "        \n",
    "    def run(self):                   #把要执行的代码写到run函数里面 线程在创建后会直接运行run函数 \n",
    "        print(\"Start drawing last month's report :\",self.bridgename,\"-\",self.notename,\"-\",self.date1,\"-\",self.date2)\n",
    "        draw_quality_map(self.bridgename,self.notename,self.date1,self.date2,self.rects)\n",
    "        print(\"Drawing down !\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  振动传感器傅里叶变换绘图线程\n",
    "class draw_fft_thread(threading.Thread):\n",
    "    def __init__(self,table_name,detect_name,begin_time,data_after_filtering):\n",
    "        try:\n",
    "            threading.Thread.__init__(self)\n",
    "            self.table_name = table_name\n",
    "            self.detect_name = detect_name\n",
    "            self.begin_time = begin_time\n",
    "            self.data_after_filtering = data_after_filtering\n",
    "        except:\n",
    "            print(\"傅里叶变换绘图线程初始化失败！\")\n",
    "            return \"wrong\"\n",
    "        \n",
    "    def run(self):\n",
    "        print(\"Start analysing data and do FFT :\")\n",
    "        # fft处理\n",
    "        data_after_fft = fft_transform(self.data_after_filtering)\n",
    "        #print(\"傅里叶变换后的数据：\",data_after_fft)\n",
    "        # 将傅里叶变换后的数据存入数据库\n",
    "        #insert_into_fft_table(conn,table_name_fft,data_after_fft[0:len(data_after_fft)//2],FREQ_REFERENCE.get(detect_name),package_number)\n",
    "        # 绘图\n",
    "        # 横坐标变换为频率\n",
    "        data_after_fft = data_after_fft[0:len(data_after_fft)//2]\n",
    "        freq = (1000000 / FREQ_REFERENCE.get(self.detect_name)) / 2\n",
    "        x = np.arange(0,len(data_after_fft)) / len(data_after_fft) * freq\n",
    "        max_index = round(np.argmax(data_after_fft)* freq / len(data_after_fft) , 4 )\n",
    "        max_value = round(max(data_after_fft) , 2 )\n",
    "        font = matplotlib.font_manager.FontProperties(fname=r\"C:\\Windows\\Fonts\\simsun.ttc\",size=12)\n",
    "        plt.rcParams['savefig.dpi'] = 120 #图片像素\n",
    "        #plt.rcParams['figure.dpi'] = 300 #分辨率\n",
    "        plt.plot(max_index,max_value,'*b')\n",
    "        show_max = '[' + str(max_index) + ',' + str(max_value) + ']'\n",
    "        plt.annotate(show_max, xytext=(max_index,max_value), xy=(max_index, max_value))\n",
    "        plt.plot(x[:],data_after_fft[:])\n",
    "        plt.xlabel(\"频率/Hz\",fontproperties=font)\n",
    "        #plt.ylabel(\"\")\n",
    "        plt.title('FFT of Mixed wave at ' + str(self.begin_time) + \" in 1 hour\",fontsize=7,color='#7A378B')  #注意这里的颜色可以查询颜色代码表\n",
    "        save_path = \"C:/Users/Administrator/Desktop/\" + self.table_name + \".jpg\"\n",
    "        plt.savefig(save_path)\n",
    "        plt.show()\n",
    "        print(\"Drawing down !\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The program is interrupted !\n",
      "Start analysing data and do FFT :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\pymysql\\cursors.py:170: Warning: (1366, \"Incorrect string value: '\\\\xD6\\\\xD0\\\\xB9\\\\xFA\\\\xB1\\\\xEA...' for column 'VARIABLE_VALUE' at row 519\")\n",
      "  result = self._query(query)\n",
      "D:\\Anaconda3\\lib\\site-packages\\pymysql\\cursors.py:170: Warning: (3719, \"'utf8' is currently an alias for the character set UTF8MB3, but will be an alias for UTF8MB4 in a future release. Please consider using UTF8MB4 in order to be unambiguous.\")\n",
      "  result = self._query(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xe2b1978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drawing down !\n",
      "The program is listenging again !\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import Column, String, create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy import and_\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "##  监听程序\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"The program is interrupted !\")\n",
    "    # 每一次获取消息，判断当前日期\n",
    "    #now_date = time.strftime(\"%Y-%m-%d\")\n",
    "    # 测试用\n",
    "    now_date = \"2019-04-01\"\n",
    "    old_date = \"2019-03-31\"\n",
    "    # 判断是否有节点信息变更\n",
    "    #if (判断信息):\n",
    "    #    # 重新读取节点表数据归类\n",
    "    #    node_info = load_node_info(\"localhost\",\"root\",\"941130zmy\",'testt',\"bus_nodeconfig\")\n",
    "    #    \n",
    "    # 后台运行程序\n",
    "    #background_running(\"C:/Users/Administrator/Desktop/SMJ_DYB-03-01_000000_003411.DY.7z\")\n",
    "    #background_running(\"C:/Users/Administrator/Desktop/SMJ_DYB-03-01_000000_003411.txt\")\n",
    "    background_running(\"C:/Users/Administrator/Desktop/SMJ_JSD-03-02_000000_005959.txt\")\n",
    "    '''\n",
    "    # 绘图判断\n",
    "    # 日绘\n",
    "    if now_date != old_date:\n",
    "        day_thread = draw_daily_report_thread(\"三门江大桥\",\"DYB-03-01\",old_date,old_date,rects=5)\n",
    "        day_thread.start()\n",
    "        #draw_quality_map(\"三门江大桥\",\"DYB-03-01\",old_date,old_date,rects=5)\n",
    "    # 月绘\n",
    "    if now_date.split(\"-\")[2] == \"01\" and now_date != old_date:\n",
    "        # 查询上月所有日内的对应文件\n",
    "        tmp_year = old_date.split(\"-\")[0]\n",
    "        tmp_month = old_date.split(\"-\")[1]\n",
    "        date_base = tmp_year + \"-\" + tmp_month + \"-\"\n",
    "        begin_date = date_base + \"01\"\n",
    "        month_num = calendar.monthrange(int(tmp_year),int(tmp_month))\n",
    "        end_date = date_base + str(month_num[1])\n",
    "        month_thread = draw_monthly_report_thread(\"三门江大桥\",\"DYB-03-01\",begin_date,end_date,rects=5)\n",
    "        month_thread.start()\n",
    "    '''\n",
    "    # 日期更新\n",
    "    old_date = now_date\n",
    "    print(\"The program is listenging again !\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Mar 21 09:06:58 2019\n",
    "\n",
    "@author: Administrator\n",
    "\"\"\"\n",
    "def ansn(table_name,bridge_name,node_name,data_after_filtering,package_name):\n",
    "    value=data_after_filtering\n",
    "    Test_type=node_name.split('_',1)[0]\n",
    "    Test_location=node_name.split('_',1)[1]\n",
    "\n",
    "    def Backfill_database(package_name,table_name):\n",
    "        Base = declarative_base()    \n",
    "        # 定义User对象                                                                         #\n",
    "        class Target(Base):\n",
    "            # 表的名字\n",
    "            __tablename__ = table_name\n",
    "            # 表的结构\n",
    "            ID = Column(String(20), primary_key=True)\n",
    "            Package_ID=Column(String(20))\n",
    "            Bridge_Name=Column(String(20))\n",
    "            Bridge_ID= Column(String(10))\n",
    "            Node_Name=Column(String(20))\n",
    "            Report_Date=Column(String(20))\n",
    "            count_sum=Column(String(10))\n",
    "            count_not_null=Column(String(10))\n",
    "            mean_quality=Column(String(10))\n",
    "            std_quality=Column(String(10))\n",
    "            max_quality=Column(String(10))\n",
    "            min_quality=Column(String(10))\n",
    "            at_0_25_percent=Column(String)\n",
    "            at_25_50_percent=Column(String)\n",
    "            at_50_75_percent=Column(String)\n",
    "            at_75_100_percent=Column(String)\n",
    "            positive_num=Column(String(10))\n",
    "            negative_num=Column(String(10))\n",
    "            positive_percent=Column(String)\n",
    "            negative_percent=Column(String)\n",
    "            abnormal_num=Column(String(10))\n",
    "            abnormal_percent=Column(String)\n",
    "            null_percent=Column(String)\n",
    "            jicha=Column(String(10))\n",
    "            zhongshu=Column(String(10))\n",
    "            skewness=Column(String(10))\n",
    "            overall_quality=Column(String(10))\n",
    "            abnormal_judge=Column(String(11))\n",
    "        engine = create_engine('mysql+pymysql://root:941130zmy@localhost:3306/bridge')\n",
    "        DBSession = sessionmaker(bind=engine)\n",
    "        # 更新\n",
    "        # 创建session\n",
    "        session = DBSession()\n",
    "        user=session.query(Target).filter((Target.Package_ID==package_name)).first()\n",
    "        user.abnormal_judge=0\n",
    "        session.commit()\n",
    "        # 关闭session\n",
    "        session.close()\n",
    "    def qurey_index(table_name,brige_name,Test_type,Test_location,value):\n",
    "        Base = declarative_base()                                                             #\n",
    "        # 定义User对象                                                                         #\n",
    "        class Target(Base):\n",
    "            # 表的名字\n",
    "            __tablename__ = 'indextable'\n",
    "            # 表的结构\n",
    "            Brige_name = Column(String(20), primary_key=True)\n",
    "            Test_type=Column(String(20))\n",
    "            Test_location=Column(String(20))\n",
    "            Upperlimit= Column(String(20))\n",
    "            Lowerlimit=Column(String(20))\n",
    "        engine = create_engine('mysql+pymysql://root:941130zmy@localhost:3306/bridge')\n",
    "        DBSession = sessionmaker(bind=engine)\n",
    "        # 查询\n",
    "        # 创建session\n",
    "        session = DBSession()\n",
    "        # 利用session创建查询，query(对象类).filter(条件).one()/all()\n",
    "        user = session.query(Target).filter(and_(Target.Brige_name==bridge_name,Target.Test_type==Test_type,Target.Test_location==Test_location)).all()\n",
    "        if len(user)==0:\n",
    "            #（AR算法）\n",
    "            print(0)\n",
    "            if (AR(value))>0.5:\n",
    "                Backfill_database(package_name,table_name)\n",
    "        else:\n",
    "            user = session.query(Target).filter(and_(Target.Brige_name==brige_name,Target.Test_type==Test_type,Target.Test_location==Test_location)).one()\n",
    "            J=0\n",
    "            upperlimit=float(user.Upperlimit)\n",
    "            lowerlimit=float(user.Lowerlimit) #user.Lowerlimit\n",
    "            \n",
    "            for i in value:   \n",
    "                if not(i>=lowerlimit and i<=upperlimit):\n",
    "                    J+=1\n",
    "                else:\n",
    "                    J-=1\n",
    "            if J>=3:\n",
    "                #insert_data(Id,Waring_info,Date,State)\n",
    "                Backfill_database(package_name,table_name)\n",
    "            else:\n",
    "                    #(AR算法)\n",
    "                print(0)\n",
    "                if (AR(value))>0.5:\n",
    "                    Backfill_database(package_name,table_name)\n",
    "        # 关闭sessionaR\n",
    "        session.close()\n",
    "    qurey_index(table_name,bridge_name,Test_type,Test_location,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def AR(x0):\n",
    "    def ar_order(x0):\n",
    "        #x0 = pd.DataFrame({\"date\":range(0,len(x0)),\"data\":x0})\n",
    "        #print(x0)\n",
    "        #pmax = int(len(x0) / 2)    \n",
    "        def ar_aic(rss,p):\n",
    "            n=len(rss)\n",
    "            s2=np.var(rss)\n",
    "            return 2*p+n*math.log(s2)\n",
    "        def ar_sc(rss,p):\n",
    "            n=len(rss)\n",
    "            s2=np.var(rss)\n",
    "            return p*math.log(n)+n*math.log(s2)    \n",
    "        aic=[]\n",
    "        sc=[]\n",
    "        for p in range(int(len(x0)/2)):\n",
    "            aic.append(ar_aic(x0,p))\n",
    "            sc.append(ar_sc(x0,p))\n",
    "        p1,p2=(aic.index(min(aic))+1,sc.index(min(aic))+1)  \n",
    "        return p1\n",
    "    #############计算AR模型参数################\n",
    "       ##########输入：\n",
    "    #############    已知序列，阶数 \n",
    "    #############输出：\n",
    "    #############    模型参数\n",
    "    def ar_parameter(sample,p):\n",
    "        sample=np.array(sample)\n",
    "        matrix_x=np.zeros([(sample.size-p),p])\n",
    "        matrix_x=np.matrix(matrix_x)\n",
    "        array=sample.reshape(sample.size)\n",
    "        j=0\n",
    "        for i in range(0,sample.size-p):\n",
    "            matrix_x[i,0:p]=array[j:j+p]\n",
    "            j=j+1\n",
    "        matrix_y=np.array(array[p:sample.size]) \n",
    "        matrix_y=matrix_y.reshape(sample.size-p,1)\n",
    "        matrix_y=np.matrix(matrix_y)\n",
    "        cofe=np.dot(np.dot((np.dot(matrix_x.T,matrix_x)).I,matrix_x.T),matrix_y)   ######AR模型参数\n",
    "        return cofe\n",
    "    ###############把数据分为已知数据并分成p1段，p2段。与未知段#########\n",
    "    ###############输入：\n",
    "    ###############      序列x0\n",
    "    ###############输出:\n",
    "    ###############      p1,p2,unknown(未知段)\n",
    "    def fenduan(x0):\n",
    "        n=len(x0)\n",
    "        p1=x0[0:int(n/3)]\n",
    "        p2=x0[int(n/3):int(2*n/3)]\n",
    "        unknown=x0[int(2*n/3):(n)]\n",
    "        return p1,p2,unknown\n",
    "    ################组成特征空间##############\n",
    "    ################输入：\n",
    "    ################    序列,p(阶数)\n",
    "    ################输出：\n",
    "    ################    由AR参数与序列方差组合得到的多个特征空间\n",
    "    def com_featurespace(x0,p):\n",
    "        feaspa=[]                 #####存放单个特征空间\n",
    "        cofevar=[]                #####存放每段的所有特征空间\n",
    "        a=0\n",
    "        while a<2:                #####假设每个段又分成2份\n",
    "            x1=x0[int(a*len(x0)/2):int((a+1)*len(x0)/2)]    \n",
    "            var=np.var(x1)\n",
    "            cofe=ar_parameter(x1,p)\n",
    "            cofe=cofe.tolist()\n",
    "            for item in cofe:\n",
    "                for i in range(len(item)):\n",
    "                    feaspa.append(item[i])\n",
    "            feaspa.append(var)\n",
    "            cofevar.append(feaspa)\n",
    "            feaspa=[]\n",
    "            a+=1\n",
    "        return cofevar\n",
    "    ##############由AR参数与序列方差组成的特征空间进行条件归一化得到特征系数#######################\n",
    "    ############## 输入：\n",
    "    ##############      p1段的特征空间，p2段或未知段的特征空间\n",
    "    ##############输出：\n",
    "    ##############      特征系数\n",
    "    def conditional_normalization(cofevarx,cofevary):\n",
    "        cofevarx=np.array(cofevarx)\n",
    "        cofevary=np.array(cofevary)\n",
    "        n=cofevarx.shape[0]\n",
    "        distance=[]              ####存放两个特征空间的欧式距离\n",
    "        feaspaindex=[]           ####存放两个特征空间的行索引\n",
    "        Df=[]                    ####存放特征系数\n",
    "        s1=10000                ####目前未知\n",
    "        s2=-10000                ####目前未知\n",
    "        for i in range(cofevary.shape[0]):\n",
    "            for j in range(cofevarx.shape[0]):\n",
    "                if abs(cofevary[i][-1]-cofevarx[j][-1])<s1:\n",
    "                    if ((cofevary[i][:n-1]).T.dot(cofevarx[j][:n-1]))/((np.linalg.norm(cofevary[i][:n-1]))*np.linalg.norm(cofevarx[j][:n-1]))>=s2:\n",
    "                            distance.append(np.linalg.norm(cofevary[i][:n-1]-cofevarx[j][:n-1]))  ###计算两个特征空间的欧氏距离\n",
    "                            feaspaindex.append([i,j])   ###并存放这两个的行索引 \n",
    "                            if j==n-1:\n",
    "                                a=distance.index(min(distance))\n",
    "                                b=feaspaindex[a][0]\n",
    "                                c=feaspaindex[a][1]\n",
    "                                Df.append((math.sqrt(cofevary[b][-1])-math.sqrt(cofevarx[c][-1]))/math.sqrt(cofevarx[b][-1]))########计算出特征系数\n",
    "                                distance=[]\n",
    "                                feaspaindex=[]\n",
    "        return Df\n",
    "    ################异常诊断#################\n",
    "    ################ 输入：\n",
    "    ################      正常结构P2特征系数，未知状态特征系数\n",
    "    ################ 输出：\n",
    "    ################      结构响应的异常概率\n",
    "    def abnormal_diagnosis(Dfn,Dfa):\n",
    "        mu=0\n",
    "        sigma=1\n",
    "        nmean=np.mean(Dfn)                       ###正常结构P2特征系数的均值\n",
    "        nvar =np.var(Dfn)                        ###正常结构P2特征系数的方差\n",
    "        amean=np.mean(Dfa)                       ###未知状态特征系数的均值\n",
    "        avar =np.var(Dfa)                        ###未知状态特征系数的方差\n",
    "        x=(nmean-amean)/(math.sqrt(nvar+avar))\n",
    "        Pa=1/(sigma*math.sqrt(2*math.pi))*math.exp(-((x-mu)**2)/(2*sigma**2))\n",
    "        return Pa \n",
    "    p1,p2,unknown=fenduan(x0)\n",
    "    p=ar_order(p1)\n",
    "    cofevarp1=com_featurespace(p1,p)\n",
    "    cofevarp2=com_featurespace(p2,p)\n",
    "    cofevaruk=com_featurespace(unknown,p)\n",
    "    Dfn=conditional_normalization(cofevarp1,cofevarp2)        \n",
    "    Dfa=conditional_normalization(cofevarp1,cofevaruk)\n",
    "    Pa= abnormal_diagnosis(Dfn,Dfa)\n",
    "    return Pa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        -32.2831\n",
      "1        -25.8707\n",
      "2        -27.0333\n",
      "3        -28.3581\n",
      "4        -25.8543\n",
      "5        -26.3390\n",
      "6        -20.2508\n",
      "7        -25.6267\n",
      "8        -29.0933\n",
      "9        -27.0661\n",
      "10       -31.5987\n",
      "11       -34.7721\n",
      "12       -36.9205\n",
      "13       -32.2062\n",
      "14       -32.8824\n",
      "15       -34.4217\n",
      "16       -32.1685\n",
      "17       -27.8766\n",
      "18       -32.5320\n",
      "19       -29.2439\n",
      "20       -28.4137\n",
      "21       -35.4795\n",
      "22       -28.6561\n",
      "23       -29.2963\n",
      "24       -30.4704\n",
      "25       -34.6673\n",
      "26       -30.7570\n",
      "27       -17.5195\n",
      "28       -20.1395\n",
      "29       -23.5422\n",
      "           ...   \n",
      "179979   -22.8168\n",
      "179980   -23.3850\n",
      "179981   -20.9435\n",
      "179982   -18.7378\n",
      "179983   -18.2040\n",
      "179984   -16.8056\n",
      "179985   -29.5698\n",
      "179986   -35.4533\n",
      "179987   -35.8741\n",
      "179988   -32.9365\n",
      "179989   -32.5189\n",
      "179990   -29.4830\n",
      "179991   -24.9373\n",
      "179992   -24.8325\n",
      "179993   -29.2423\n",
      "179994   -33.4850\n",
      "179995   -40.1398\n",
      "179996   -40.3805\n",
      "179997   -35.4271\n",
      "179998   -34.9178\n",
      "179999   -22.1733\n",
      "180000   -16.7515\n",
      "180001   -23.7272\n",
      "180002   -23.4030\n",
      "180003   -29.0982\n",
      "180004   -39.3342\n",
      "180005   -39.2163\n",
      "180006   -39.5732\n",
      "180007   -31.5381\n",
      "180008   -23.5127\n",
      "Name: A, Length: 180009, dtype: float64\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Given a pandas object and the index does not contain dates",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py\u001b[0m in \u001b[0;36m_init_dates\u001b[1;34m(self, dates, freq)\u001b[0m\n\u001b[0;32m     54\u001b[0m         super(TimeSeriesModel, self).__init__(endog, exog, missing=missing,\n\u001b[1;32m---> 55\u001b[1;33m                                               **kwargs)\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-123-134c8a83f47b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"A\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mx0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"A\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mAR\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-122-541ec1c7de52>\u001b[0m in \u001b[0;36mAR\u001b[1;34m(x0)\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mPa\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[0mp1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mp2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0munknown\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfenduan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m     \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mar_order\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m     \u001b[0mcofevarp1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcom_featurespace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[0mcofevarp2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcom_featurespace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-122-541ec1c7de52>\u001b[0m in \u001b[0;36mar_order\u001b[1;34m(x0)\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mtemp\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[1;31m#try:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0mtemp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mARMA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbic_min_order\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m             \u001b[1;31m#except:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[1;31m#    temp.append(None)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\arima_model.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, endog, order, exog, dates, freq, missing)\u001b[0m\n\u001b[0;32m    450\u001b[0m         \"\"\"\n\u001b[0;32m    451\u001b[0m         \u001b[0mGet\u001b[0m \u001b[0mstarting\u001b[0m \u001b[0mparameters\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m         \u001b[0mParameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m         \u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, endog, exog, dates, freq, missing)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[0m_generic_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model_params_doc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[0m_missing_param_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_missing_param_doc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mTimeSeriesModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLikelihoodModel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py\u001b[0m in \u001b[0;36m_init_dates\u001b[1;34m(self, dates, freq)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;31m# Date handling in indexes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_dates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_init_dates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Given a pandas object and the index does not contain dates"
     ]
    }
   ],
   "source": [
    "f = open(r\"C:\\Users\\Administrator\\Desktop\\柳州桥梁BDM项目\\柳州维义桥原始数据\\振动\\JSD-03-01_000000_005959.txt\")\n",
    "data = pd.read_csv(f,names=[\"A\",\"B\"])\n",
    "print(data[\"A\"])\n",
    "x0=data[\"A\"]\n",
    "AR(x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
